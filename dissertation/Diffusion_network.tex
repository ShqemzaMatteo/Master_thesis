\section{Random walk on networks}
Let consider a network $G(N,E)$ made by $N$ vertices or nodes connected by $E$ links.
Consider that in the graph there is a particle and, at each time step, it moves randomly between the nodes, with transition probability $P_{ij}$ to go to the node $j$ starting from the node $i$; if there is no link between them $P_{ij}= 0$. 
The dynamics behave as a Markov chain, it has no memory of the previous states. 
Let be $\rho_i (t)$ the probability to find the particle in the node $i$ at time step $n$, the discrete time evolution of the system is given by the law
\begin{equation}
    \rho_i(n+1) = \sum_j P_{ij}\rho_j(n).
\end{equation}

In order to conserve the total probability the transition probability must be a stochastic matrix, namely it must hold 
\begin{equation}
    \sum_i P_{ij}(\Delta t) = 1 .
\end{equation}

The transition probability  can be identify with the adjacency matrix of the network
\begin{equation}
    P_{ij} = \frac{A_{ij}}{\sum_j A_{ij}}.
\end{equation}

If the system obeys the detail balance, $\pi_{ij} \rho_j^* = \pi_{ji} \rho_i^*$, the system admits a unique stationary solution $\rho^*$ such that
\begin{equation}
    \sum_j P_{ij}\rho^*_j =  \rho^*_i .
\end{equation}
We can rewrite it in a matrix formalism where $\Pi$ is the matrix of the transition probability, and $\rho^*(t)$ is the stationary probability vector, as
\begin{equation}
    \Pi \rho^* = \rho^*.
\end{equation}
It can be seen that the stationary distribution is the eigenvector with eigenvalue $1$. 


We cqn go in the continuum limit obtaining the master equation equation \cite{Classic_random_walk}
\begin{equation}\label{master_eq}
    \dot \rho_i(t) = \sum_j \pi_{ij}\rho_j(t) - \pi_{ji}\rho_i(t) = - \sum_j L_{ij} \rho_j(t),
\end{equation}
where $\pi_{ij} = A_{ij}$ is the transition rate, namely the transition probability per units of time, and $L_{ij} = \sum_k \pi_{kj}\delta_{ij} -\pi_{ij} $ is the Laplacian matrix.
The first term indicates that a particle is coming in the node $i$ from $j$ and the second one, instead, indicates that a particle is going into node $j$ from $i$.

The Laplacian matrix has the property that $L_{ij} < 0 $ for $i \neq j$ and also it must hold the relation
\begin{equation}
    \sum_i L_{ij} = 0 .
\end{equation} 

The eigenvalue of the Laplacian matrix has always a not negative real part.
Its spectrum has at least one zero eigenvalue, therefore it is not invertible \cite{Boccaletti}. The multiplicity of the zero eigenvalue is equal to the number of connected component of the network; it derived from the fact that if the network is not connected the Laplacian should be a block matrix, one for each connected component, and each block should have its zero eigenvalue, since each block can be seen as an independent network.

The master equation \eqref{master_eq} has a solution
\begin{equation}\label{random_walk_solution}
    \rho(t) = e^{-tL}\rho(0).
\end{equation}

The master equation, in the matrix formalism, for the stationary distribution reduces to 
\begin{equation}\label{stationary_distribution}
    \dot \rho^*(t) = -L \rho^*(t) = 0 . 
\end{equation}
As a matter of fact the stationary distribution is the eigenvector with eigenvalue $0$ of the Laplacian matrix. The other eigenvalues are connected to the Ljapunov exponent and to the time that the occurs to converge to $\rho^*$.

We can prove that
\begin{equation}
    \sum_i \dot\rho_i(t) = - \sum_i \sum_j L_{ij} \rho_j(t) = - \sum_j \left(\sum_i L_{ij}\right) \rho_j(t) = 0 .
\end{equation}
Therefore we have a first integral of motion 
\begin{equation}
    \sum_i \rho_i(t) = \sum_i \rho_i(0) .
\end{equation}

%%add the part with the measure of the eigenvalue, i.e. sum_{\lamda \neq 0} v_\lambda(t) = 0

Let suppose that the network satisfy the detail condition, namely
\begin{equation}
    \pi_{ij}p^*_j = \pi_{ji}p^*_i,
\end{equation}
then exist a hyperplane $\Sigma_0$ that is orthogonal to the stationary distribution is an invariant subspace for the dynamics. Let be $w \in \Sigma_0$, this subspace is identify by the relation
\begin{equation}
    \sum_i w_i = 0 
\end{equation}
As a matter of fact, let $w(t) \in \Sigma_0$ then 
\begin{equation}
    \sum_i w_i(t+1) = \sum_{ij} \pi_{ij} w_j(t) = \sum_j \underbrace{\left(\sum_i \pi_{ij}\right)}_{1} w_j = \sum_j w_j(t) = 0.
\end{equation}

Therefore is possible to decompose any probability vector as a direct sum of the stationary state and the invariant subspace. Let $w(t) \in \Sigma_0$ and $c = \sum_i v_i = 1$ then 
\begin{equation}
    \rho(t) = \rho^* + w(t)
\end{equation}

Besides, if the detail condition holds, the stationary distribution  is
\begin{equation}
    \rho^* = \frac{1}{N} \left(1,1,\cdots,1,1\right).
\end{equation} 

\begin{comment}
It maximizes the Gibbs entropy
\begin{equation}
    S = -\sum_i p_i\ln p_i.
\end{equation}


In figure \ref{fig:node_0} is shown only the probability to be in the initial node as a function of time starting  from a Dirac delta distribution for different types of networks\footnote{The python scripts can be found in the GitHub page of the author at the link: \url{https://github.com/ShqemzaMatteo/Master_thesis}}: a ring graph, an Erd\H{o}s-Rényi (E-R) random graph\cite{erdos-renyi1960}, a Barab\'asi-Albert (B-A) scale-free graph\cite{Barabasi_Albert_1999}, and a Watts-Strogatz (W-S) small-sworld graph\cite{Watts-Strogatz_1998}. All the algorithm are explained in the Appendix \Ref{Appendix_A}. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.65\linewidth]{image/random_graph_node_0.png}
    \caption{Plot of the probability $p$ to be in the node 0 starting from a Dirac delta distribution or the same node as a function of time for different network types of $50$ nodes: a ring graph (blue), a Erd\H{o}s-Rényi (E-R) random graph with connectivity probability $0.7$ (orange), a Barab\'asi-Albert (B-A) scale-free graph with parameter $m=3$ (green), and a Watts-Strogatz (W-S) small world graph with parameter $K=3$ and rewire probability 0.2 (red).}
    \label{fig:node_0}
\end{figure}
\end{comment}